# -*- coding: utf-8 -*-
"""RudraMLModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mOz-7jcsp7o2Hg_QJlr9yIr9szGpxCAW
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import shutil
from sklearn.model_selection import train_test_split

# Define the path to your original dataset directory
original_dataset_dir = '/content/drive/MyDrive/dataset sih/train/Eczema Photos'

# Define the directory where you want to create train and test splits
base_dir = '/content/drive/MyDrive/dataset sih/train'
os.makedirs(base_dir, exist_ok=True)

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix

# Define directories for train and test data
train_dir = '/content/drive/MyDrive/dataset%sih/train'
test_dir = '/content/drive/MyDrive/dataset%sih/test'

# Define image dimensions and batch size
img_width, img_height = 224, 224
batch_size = 2

# Create data generators with data augmentation for training and validation
train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

test_datagen = ImageDataGenerator(rescale=1.0/255.0)

try:
    train_generator = train_datagen.flow_from_directory(   #forming labels
    directory="/content/drive/MyDrive/dataset sih/train",
    target_size=(224, 224),
    color_mode="rgb",
    batch_size=22,
    class_mode="categorical",seed=42



)


    test_generator = test_datagen.flow_from_directory(
    directory="/content/drive/MyDrive/dataset sih/test",
    target_size=(224, 224),
    color_mode="rgb",
    batch_size=3,
    class_mode=None,seed=42

)
    valid_generator = train_datagen.flow_from_directory(
    directory="/content/drive/MyDrive/dataset sih/train",
    target_size=(224, 224),
    color_mode="rgb",
    batch_size=8,
    #subset='validation',
    shuffle=True,
    seed=42,
    class_mode="categorical")



    # Continue with the rest of the code (model creation, compilation, training, evaluation)

except FileNotFoundError as e:
    print("Error: Directory not found. Please check the path to your dataset.")
except Exception as e:
    print(f"An error occurred: {str(e)}")


# Create a pre-trained ResNet50 model with a custom top layer
base_model = ResNet50(weights='imagenet', include_top=False)

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='sigmoid')(x)
predictions = Dense(len(train_generator.class_indices), activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

print(len(train_generator))
print(len(test_generator))
print(len(valid_generator))

def prepare_model():
    model = Sequential()
    model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(224, 224, 3)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(16, activation='sigmoid'))
    model.add(Dense(2, activation='sigmoid'))
    model.compile(loss="binary_crossentropy",optimizer="adam",metrics=['accuracy'])
    return model

for batch in train_generator:
    x_batch, y_batch = batch  # Unpack the input and target (label) batches
    print(f"Input batch shape: {x_batch.shape}")
    print(f"Target batch shape: {y_batch.shape}")
    break  # Break after processing one batch

print(f"Training batch size: {train_generator.batch_size}")
print(f"Validation batch size: {valid_generator.batch_size}")

model = prepare_model()

# Debugging: Print some information to check data shapes
print(f"Number of samples in training generator: {train_generator.samples}")
print(f"Number of samples in validation generator: {valid_generator.samples}")
print(f"Steps per epoch: {train_generator.samples // train_generator.batch_size}")
print(f"Validation steps: {valid_generator.samples // valid_generator.batch_size}")

# Train the model
history = model.fit(
    train_generator,
    validation_data=valid_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_steps=valid_generator.samples // valid_generator.batch_size,
    epochs=5
)

# Plot training history
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

score = model.evaluate(valid_generator)
print('Test loss:', score[0])
print('Test accuracy:',score[1])

predict=model.predict(test_generator)
# predict the class label
y_classes = predict.argmax(axis=-1)

